{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "name": "mistral_quant_awq_load_local_RAG.ipynb",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aswinaus/LLM_Inference/blob/main/mistral_quant_awq_load_local_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbxi59eeGs7b"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers torch accelerate langchain langchain_huggingface datasets autoawq --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai langchain_community chromadb tiktoken --quiet"
      ],
      "metadata": {
        "id": "v3hEx_PFKBVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code is essentially forcing Python to always use \"UTF-8\" as the preferred encoding, regardless of the user's actual system settings. UTF-8 is a widely used encoding that can represent a vast range of characters from different languages. By enforcing UTF-8, you can help ensure that your code works consistently across different platforms and avoids encoding-related errors. It's a common practice for improving compatibility and preventing issues with text handling in Python programs."
      ],
      "metadata": {
        "id": "vpCeli-FI8me"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "59dUt6OwIvGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "c5-4gG8tIrFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "HUGGING_FACE_TOKEN = userdata.get('HUGGING_FACE_TOKEN')"
      ],
      "metadata": {
        "id": "UGGq_MH0LK3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token $HUGGING_FACE_TOKEN"
      ],
      "metadata": {
        "id": "6aIoujSHLcnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Download Data\n",
        "data_dir = '/content/drive/MyDrive'"
      ],
      "metadata": {
        "id": "R1y8bIH8PmnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from threading import Thread"
      ],
      "metadata": {
        "id": "ML6AIgxOLraR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The nvidia-smi command is a utility provided by NVIDIA to query and display information about your NVIDIA GPU(s) (Graphics Processing Unit). This includes things like:\n",
        "\n",
        "GPU model and name\n",
        "Driver version\n",
        "GPU utilization\n",
        "Memory usage\n",
        "Temperature\n",
        "Power consumption\n",
        "Processes running on the GPU"
      ],
      "metadata": {
        "id": "Im62RU0RMXkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "GHtMY71ZL6tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def wrap_text(text, width=90): #preserve_newlines\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text"
      ],
      "metadata": {
        "id": "58tu7qi6N1ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from awq import AutoAWQForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "from typing import Tuple, Optional, Union, Dict, Any\n",
        "from transformers import PreTrainedModel, AutoModel, AutoTokenizer, AutoConfig\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "AftfSEoxBbzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/MyDrive' # Input a data dir path from your mounted Google Drive"
      ],
      "metadata": {
        "id": "XpQJkOZdmeZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant_path = f\"/{data_dir}/LLMs/Mistral/Mistral-Small-24B-Instruct-2501\""
      ],
      "metadata": {
        "id": "w3Uz7PsddoAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MistralForCausalLM, AutoTokenizer\n",
        "local_model_path = quant_path\n",
        "local_tokenizer = AutoTokenizer.from_pretrained(quant_path)\n",
        "local_model = MistralForCausalLM.from_pretrained(quant_path,low_cpu_mem_usage=True)"
      ],
      "metadata": {
        "id": "vaNKVuYhdrK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "local_model.to(device) moves all the model's parameters and buffers to the specified device (in this case, device, which is set to 'cuda' if a GPU is available). Deep learning models often have a large number of parameters and require significant computational power. GPUs are designed for parallel processing and can significantly speed up the training and inference of deep learning models. By moving the model to the GPU, you leverage its computational capabilities for faster execution."
      ],
      "metadata": {
        "id": "_jGUsY3gQz-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "local_model.to(device)"
      ],
      "metadata": {
        "id": "3KdRroiQQoWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "dataset = load_dataset(\"aswinaus/tax_statistics_dataset_by_income_range\", download_mode=\"force_redownload\")\n",
        "df=pd.DataFrame(dataset['train'])\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "cuahtSIGlWMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG pipeline implementation"
      ],
      "metadata": {
        "id": "mKNttnRdH8F9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "from langchain_core.runnables import (\n",
        "    RunnableParallel,\n",
        "    RunnablePassthrough\n",
        ")\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from typing import Dict, Any, List\n",
        "from langchain.docstore.document import Document"
      ],
      "metadata": {
        "id": "dnGjawgrIDsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "xWavq4inLVyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "fsoiB2yqRSrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RecursiveCharacterTextSplitter for splitting the documents into chunk size and overlapp for efficient meaningful chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50,\n",
        ")"
      ],
      "metadata": {
        "id": "W8AnvVl2R6-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define a pad token for tokenizer and save it.\n",
        "#local_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "#local_model.resize_token_embeddings(len(local_tokenizer))\n",
        "#local_tokenizer.save_pretrained(quant_path)\n",
        "#local_model = AutoModel.from_pretrained(quant_path,low_cpu_mem_usage=True, trust_remote_code=True)\n",
        "#local_model.to(device)"
      ],
      "metadata": {
        "id": "ZnnNfLniUDuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import transformers # This line imports the transformers module.\n",
        "import torch\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from threading import Thread"
      ],
      "metadata": {
        "id": "LdaBKkgWHJsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code creates a text generation pipeline that utilizes a specified local pre-trained model and tokenizer to generate text, with parameters controlling the randomness, repetition, length, and format of the generated output."
      ],
      "metadata": {
        "id": "b-0gPYV3qyaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_generation_pipeline = transformers.pipeline(\n",
        "    model=local_model,#line specifies the pre-trained language model to be used for text generation. The local_model variable holds a local trained model object that was previously loaded.\n",
        "    tokenizer=local_tokenizer,#specifies the tokenizer to be used. A tokenizer is responsible for breaking down the input text into individual tokens (words or subwords) that the model can understand. local_tokenizer likely holds a tokenizer object that was previously loaded and corresponds to the chosen local_model.\n",
        "    task=\"text-generation\",  # Specify the task as text generation\n",
        "    temperature=0.3,  # Temperature parameter for controlling randomness in sampling\n",
        "    repetition_penalty=1.1,  # Repetition penalty parameter to avoid repeating tokens\n",
        "    return_full_text=True,  # Flag to return full text instead of a list of generated tokens\n",
        "    max_new_tokens=1000,  # Maximum number of tokens to generate\n",
        "    do_sample=True  # Flag to use sampling during text generation\n",
        ")\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "### [INST]\n",
        "Instruction: I will ask you a QUESTION and give you a CONTEXT and you will respond with an answer easily understandable.\n",
        "\n",
        "### CONTEXT:\n",
        "{context}\n",
        "\n",
        "### QUESTION:\n",
        "{question}\n",
        "\n",
        "[/INST]\n",
        " \"\"\"\n",
        "\n",
        "#RAG Pipline\n",
        "\n",
        "# Create HuggingFacePipeline object wrapping the text generation pipeline\n",
        "Huggingface_mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
        "\n",
        "# Create prompt object from the prompt template with input variables as context and question\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],  # Specify input variables for the prompt\n",
        "    template=prompt_template,  # Specify the template for the prompt\n",
        ")\n",
        "\n",
        "# Create language model chain (llm_chain) with HuggingFacePipeline and prompt\n",
        "llm_chain = prompt | Huggingface_mistral_llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "jiCIqIhpD52T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "llm_prompt_base = \"\"\"\n",
        "### [INST]\n",
        "Instruction: You are a tax assistant\n",
        "\n",
        "\n",
        "### QUESTION:\n",
        "What is Tax Form 990?\n",
        "\n",
        "[/INST]\n",
        " \"\"\""
      ],
      "metadata": {
        "id": "0DOtuqysIOZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=text_generation_pipeline(llm_prompt_base)\n",
        "#r=response[0].get('generated_text').split(\"[/INST]\")[1].split('')[0]\n",
        "#print(r.strip())"
      ],
      "metadata": {
        "id": "Ge_qKP4tIcGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "id": "7e1ouI6xIc9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assume 'df' is your Pandas DataFrame\n",
        "train_ratio = 0.8  # Proportion of data for training\n",
        "validation_ratio = 0.1  # Proportion of data for validation\n",
        "test_ratio = 0.1  # Proportion of data for testing\n",
        "\n",
        "# Shuffle the DataFrame (optional but recommended)\n",
        "df = df.sample(frac=1, random_state=42)\n",
        "\n",
        "# Calculate split indices\n",
        "train_index = int(train_ratio * len(df))\n",
        "validation_index = int((train_ratio + validation_ratio) * len(df))\n",
        "\n",
        "# Split the DataFrame\n",
        "train_df = df[:train_index]\n",
        "validation_df = df[train_index:validation_index]\n",
        "test_df = df[validation_index:]\n",
        "\n",
        "print(f\"Training set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(validation_df)}\")\n",
        "print(f\"Testing set size: {len(test_df)}\")"
      ],
      "metadata": {
        "id": "HaNtcWG-l-0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "database_path = f\"{data_dir}/RAG/VectorDB/chroma_db_RAG_quantnew\"\n",
        "\n",
        "\n",
        "# Delete the existing database\n",
        "database_path = f\"{data_dir}/RAG/VectorDB/chroma_db_RAG_quantnew\"  # Update with your actual database path\n",
        "if os.path.exists(database_path):\n",
        "    shutil.rmtree(database_path)\n",
        "\n",
        "# Create the database directory\n",
        "os.makedirs(database_path, exist_ok=True)\n",
        "\n",
        "# Initialize Chroma client\n",
        "client = chromadb.Client()\n",
        "\n",
        "# Define embedding function (choose one or configure your own)\n",
        "# Default Sentence Transformers embedding function\n",
        "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction()\n",
        "\n",
        "# Create a collection (database table)\n",
        "collection_name = \"my_table_collection\"\n",
        "collection = client.get_or_create_collection(name=collection_name, embedding_function=sentence_transformer_ef)\n",
        "\n",
        "# Chunking and storing data\n",
        "chunk_size = 1 # Process one row at a time\n",
        "for i in range(0, len(df), chunk_size):\n",
        "    chunk = df.iloc[i:i + chunk_size]\n",
        "    chunk_text = chunk.to_string() # Convert chunk to string\n",
        "    ids = [f\"row_{i+j}\" for j in range(len(chunk))] # Create unique ids for each row\n",
        "    collection.add(\n",
        "        documents= [chunk_text],\n",
        "        ids=ids\n",
        "    )\n",
        "\n",
        "#Example query\n",
        "results = collection.query(\n",
        "    query_texts=[\"value 2\"],\n",
        "    n_results=1\n",
        ")\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "5IK3_isy8EX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9zqzfX3KYjDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import shutil\n",
        "class MyEmbedding:\n",
        "    def __init__(self, model):\n",
        "        self.model = SentenceTransformer(model, trust_remote_code=True)\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        return [self.model.encode(text).tolist() for text in texts]\n",
        "\n",
        "    def embed_query(self, query: str) -> List[float]:\n",
        "        encoded_query = self.model.encode(query)\n",
        "        return encoded_query.tolist()\n",
        "\n",
        "database_path = f\"{data_dir}/RAG/VectorDB/chroma_db_RAG_quantnew\"\n",
        "\n",
        "\n",
        "# Delete the existing database\n",
        "database_path = f\"{data_dir}/RAG/VectorDB/chroma_db_RAG_quantnew\"  # Update with your actual database path\n",
        "if os.path.exists(database_path):\n",
        "    shutil.rmtree(database_path)\n",
        "\n",
        "# Create the database directory\n",
        "os.makedirs(database_path, exist_ok=True)\n",
        "\n",
        "def generate_embeddings(data):\n",
        "    embeddings = []\n",
        "    for text in df.head(10):\n",
        "        text_splitter =  RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=10)\n",
        "        chunks = text_splitter.create_documents(text)\n",
        "\n",
        "        # Get embeddings for each chunk using local_model\n",
        "        chunk_embeddings = []\n",
        "        for chunk in chunks:\n",
        "            encoded_input = local_tokenizer(chunk.page_content, return_tensors=\"pt\").to(device)\n",
        "            embedding = local_model(**encoded_input)[0].detach().cpu().numpy()\n",
        "            chunk_embeddings.append(embedding)\n",
        "\n",
        "        # Use an average or other aggregation method if necessary\n",
        "        # For example, averaging the embeddings:\n",
        "        # embedding = np.mean(chunk_embeddings, axis=0)\n",
        "\n",
        "        chromadb = Chroma.from_documents(chunks,\n",
        "                                 persist_directory=database_path,\n",
        "                                 collection_name='coll_cosine',\n",
        "                                 collection_metadata={\"hnsw:space\": \"cosine\"},\n",
        "                                 embedding=MyEmbedding(model=local_model_path))\n",
        "\n",
        "        chromadb.persist()\n",
        "        retriever = chromadb.as_retriever()\n",
        "    return embeddings  # Return a list of embeddings or any desired output\n"
      ],
      "metadata": {
        "id": "FLj1V9JEwCKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean Unused Tensors\n",
        "torch.cuda.empty_cache()\n",
        "generate_embeddings(dataset)"
      ],
      "metadata": {
        "id": "AGB4pL-EIOAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input your question in string type and the relative path of your own local model.\n",
        "def retrieve(user_query, model_path):\n",
        "    embedding_model = MyEmbedding(model_path)\n",
        "\n",
        "    chromadb = Chroma(embedding_function=embedding_model,\n",
        "                      collection_name='coll_cosine',\n",
        "                      collection_metadata={\"hnsw:space\": \"cosine\"},\n",
        "                      persist_directory=database_path)\n",
        "\n",
        "    results = chromadb.similarity_search_with_score(user_query, 10)\n",
        "    print(results)\n",
        "    return results[0][0].page_content"
      ],
      "metadata": {
        "id": "rmOcdS4CabRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieve(\"For the State of AL can you get me the No of returns for Size of adjusted gross income $50,000 under $75,000\",local_model_path)"
      ],
      "metadata": {
        "id": "EW6IC1j9aeMF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}