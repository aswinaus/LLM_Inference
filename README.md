# LLM_Inference
LLM Inference with RAG and Evaluate


# Following are some of the Inference Metrics which has been demonstrated.

**Blog :** https://aswin.ai/portfolio/blogs/llm-inference 

**Notebook:** https://github.com/aswinaus/LLM_Inference/blob/main/RAG_Evaluation_Inference_Local_Model_and_OpenAI_GPU.ipynb#

**Measure time for the first token**

**Measure time for each token**

**Calculate end-to-end latency**

**Calculate time to first token**

**Calculate inter-token latency**

**Calculate throughput**
